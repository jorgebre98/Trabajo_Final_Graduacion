{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7a6ea6",
   "metadata": {},
   "source": [
    "# ************************************************************* #\n",
    "#               Copyright (C) 2022 Jorge Brenes Alfaro.\n",
    "#               EL5617 Trabajo Final de Graduación.\n",
    "#               Escuela de Ingeniería Electrónica.\n",
    "#               Tecnológico de Costa Rica.\n",
    "# ************************************************************* #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68c6f1",
   "metadata": {},
   "source": [
    "This file is responsible for generating the mimetic neural network (MNN). First, the data collected from the PAHM is processed, which is reshaped as necessary for the network. Next, the model is developed using recurrent neural networks (RNN), specifically the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee822fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: jorgebre98 (mimetic-rna). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Libraries to proccess data\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a987f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries to proccess data\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Libraries to create RNAM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU, TimeDistributed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e874352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:Probando) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Probando</strong>: <a href=\"https://wandb.ai/mimetic-rna/Prueba/runs/Probando\" target=\"_blank\">https://wandb.ai/mimetic-rna/Prueba/runs/Probando</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221002_165922-Probando\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:Probando). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362b4c4c25b54c2d9fd6a02731724e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.033426515261332196, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\jorge\\Documents\\TEC\\TFG\\Trabajo_Final_Graduacion\\wandb\\run-20221002_170204-Probando</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/mimetic-rna/Prueba/runs/Probando\" target=\"_blank\">Probando</a></strong> to <a href=\"https://wandb.ai/mimetic-rna/Prueba\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Prueba\", \n",
    "           entity=\"mimetic-rna\", \n",
    "           name='Probando',\n",
    "           resume='Allow', \n",
    "           id='Probando')\n",
    "wandb.config = {\n",
    "    \"epochs\": 500,\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"window\": 100,\n",
    "    \"Dropout\": 0.35,\n",
    "    \"n_layers\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8faef9",
   "metadata": {},
   "source": [
    " Process the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5bd25d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* Process the Dataset *******************\n",
      "Recolecting Data\n",
      "Accommodating data for the GRU network\n",
      "Separating data in training, validation and testing\n",
      "El total de datos de entrenamiento es:  82911\n",
      "El total de datos de validación es:  27637\n",
      "El total de datos de prueba es:  27638\n",
      "Reshape arrays to tensors\n",
      "******************* Finish *******************\n"
     ]
    }
   ],
   "source": [
    "root = '/Users/jorge/Documents/TEC/TFG/Datos_Recolectados/'\n",
    "Dir = os.listdir(root)\n",
    "pwm = np.array([])\n",
    "angle = np.array([])\n",
    "\n",
    "# Read all the .csv files and make an nx4 array\n",
    "# Next, separate the pwm value and angle in their respective arrays.\n",
    "print('******************* Process the Dataset *******************',flush=True)\n",
    "print('Recolecting Data',flush=True)\n",
    "for filename in Dir:\n",
    "    files = pd.read_csv(root+filename)\n",
    "    pwm = np.append(pwm, np.concatenate((np.zeros(100),files.values[:,2])))\n",
    "    angle = np.append(angle, np.concatenate((np.zeros(100),files.values[:,3])))\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "window = 100\n",
    "\n",
    "#For each element of training set, we have \"window\" previous training set elements\n",
    "print('Accommodating data for the GRU network',flush=True)\n",
    "for i in range(window,pwm.shape[0]):\n",
    "    X_train.append(pwm[i-window:i])\n",
    "    Y_train.append(angle[i])\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train) # Input and output arrays\n",
    "\n",
    "# Separate the values in train, validation and test data/label\n",
    "train_data, val_data, test_data = [],[],[]\n",
    "train_label, val_label, test_label = [],[],[]\n",
    "train_lenght = int(len(X_train)*3/5)\n",
    "val_lenght = int(len(X_train)*4/5)\n",
    "\n",
    "# Use 3/5 of the total data set for training \n",
    "# and 1/5 for validation and testing.\n",
    "print('Separating data in training, validation and testing',flush=True)\n",
    "for i,j in zip(X_train[:train_lenght],Y_train[:train_lenght]):\n",
    "    train_data.append(i)\n",
    "    train_label.append(j)\n",
    "\n",
    "for i,j in zip(X_train[train_lenght:val_lenght],Y_train[train_lenght:val_lenght]):\n",
    "    val_data.append(i)\n",
    "    val_label.append(j)\n",
    "    \n",
    "for i,j in zip(X_train[val_lenght:],Y_train[val_lenght:]):\n",
    "    test_data.append(i)\n",
    "    test_label.append(j)\n",
    "    \n",
    "train_data, val_data, test_data = np.array(train_data), np.array(val_data), np.array(test_data)\n",
    "train_label, val_label, test_label = np.array(train_label), np.array(val_label), np.array(test_label)\n",
    "\n",
    "print('El total de datos de entrenamiento es: ', len(train_data), flush=True)\n",
    "print('El total de datos de validación es: ', len(val_data), flush=True)\n",
    "print('El total de datos de prueba es: ', len(test_data), flush=True)\n",
    "\n",
    "# Reshape the arrays (n,window,1). Where n is the total amount of data in the array\n",
    "print('Reshape arrays to tensors',flush=True)\n",
    "train_data = np.reshape(train_data,(train_data.shape[0],train_data.shape[1],1))\n",
    "val_data = np.reshape(val_data,(val_data.shape[0],val_data.shape[1],1))\n",
    "test_data = np.reshape(test_data,(test_data.shape[0],test_data.shape[1],1))\n",
    "print('******************* Finish *******************',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03e99355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[1520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5672953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82911, 100, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63423229",
   "metadata": {},
   "source": [
    "******************* Neural Network *******************\n",
    "Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ad37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(64, input_shape=(X_train.shape[1],1),return_sequences=True))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(GRU(64, input_shape=(X_train.shape[1],1),return_sequences=True))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(TimeDistributed(Dense(1))) # There is no difference between this and model.add(Dense(1))...\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mse','acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59b4162",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091be107",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_label,\n",
    "                    epochs=10, batch_size=8,\n",
    "                    validation_data = (val_data,val_label),\n",
    "                    verbose=2,callbacks=[WandbCallback(save_model=False)])\n",
    "# Prediction\n",
    "testPredict = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d7a11",
   "metadata": {},
   "source": [
    "Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b35b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb5443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(model, 'GRU_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151d16e",
   "metadata": {},
   "source": [
    "Plot the predicted and \"true\" output and plot training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEngo que graficar real vs predicha\n",
    "plt.figure()\n",
    "plt.plot(tlabel[0:100])\n",
    "plt.plot(testPredict[1])\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(1,len(loss)+1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss,'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss,'r', label='Validation loss')\n",
    "plt.title('Training and validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb9480",
   "metadata": {},
   "source": [
    "# GRU con Pythorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalación de pythorch\n",
    "!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819229bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, output_dim, drop_prob=0.2):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "        #self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.output_dim,x.size(0),self.hidden_size).to{device}\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        #out = self.fc(self.relu(out[:,-1]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a05d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUNetwork()\n",
    "print(model.parameters())\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model.params(), lr=wandb.config['learning_rate'])\n",
    "hystory = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8367bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, wandb.config[\"Epochs\"] + 1):\n",
    "    y_train = model(train_data)\n",
    "    loss = loss_function(input=y_train, target=train_label)\n",
    "    loss.backward() # Backward propagation. Calculate all the gradients needed to adjust the weights.\n",
    "    optimizer.step() # Uses gradients to change weight values\n",
    "    optimizer.zero_grad() # Don't accumulate gradients in epoch iterations.\n",
    "    \n",
    "    # Calculate the accuracy without modifying the weights of the neural network\n",
    "    with torch.no_grad():\n",
    "        y_train = model(train_data)\n",
    "        correct = (y_train == train_label).sum()\n",
    "        accuracy_train = 100*correct/float(len(test_data))\n",
    "    \n",
    "    df_temp = pd.DataFrame(data={\n",
    "        'Epoch': epoch,\n",
    "        'Loss': round(loss.item(),5),\n",
    "        'Accuracy': round(accuracy.item(),5)},index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, wandb.config[\"Epochs\"] + 1):\n",
    "    y_predict = model(test_data)\n",
    "    loss = loss_function(input=y_predict, target=test_label)\n",
    "    loss.backward() # Backward propagation. Calculate all the gradients needed to adjust the weights.\n",
    "    optimizer.step() # Uses gradients to change weight values\n",
    "    optimizer.zero_grad() # Don't accumulate gradients in epoch iterations.\n",
    "    \n",
    "    # Calculate the accuracy without modifying the weights of the neural network\n",
    "    with torch.no_grad():\n",
    "        y_predict = model(test_data)\n",
    "        correct = (y_predict == test_data).sum()\n",
    "        accuracy = 100*correct/float(len(test_data))\n",
    "    \n",
    "    df_temp = pd.DataFrame(data={\n",
    "        'Epoch': epoch,\n",
    "        'Loss': round(loss.item(),5),\n",
    "        'Accuracy': round(accuracy.item(),5)},index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30347bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d79ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa59f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648d619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
