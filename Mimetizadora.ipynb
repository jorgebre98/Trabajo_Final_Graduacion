{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7a6ea6",
   "metadata": {},
   "source": [
    "# ************************************************************* #\n",
    "#               Copyright (C) 2022 Jorge Brenes Alfaro.\n",
    "#               EL5617 Trabajo Final de Graduación.\n",
    "#               Escuela de Ingeniería Electrónica.\n",
    "#               Tecnológico de Costa Rica.\n",
    "# ************************************************************* #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68c6f1",
   "metadata": {},
   "source": [
    "This file is responsible for generating the mimetic neural network (MNN). First, the data collected from the PAHM is processed, which is reshaped as necessary for the network. Next, the model is developed using recurrent neural networks (RNN), specifically the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a987f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries to proccess data\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Libraries to create RNAM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU, TimeDistributed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8faef9",
   "metadata": {},
   "source": [
    " Process the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd25d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dir = os.listdir('/Users/jorge/Documents/TEC/TFG/Datos_Recolectados')\n",
    "\n",
    "pwm = np.array([])\n",
    "angle = np.array([])\n",
    "\n",
    "# Read all the .csv files and make an nx4 array\n",
    "# Next, separate the pwm value and angle in their respective arrays.\n",
    "for filename in Dir:\n",
    "    files = pd.read_csv('/Users/jorge/Documents/TEC/TFG/Datos_Recolectados/'+filename)\n",
    "    pwm = np.append(pwm,np.zeros(100))\n",
    "    pwm = np.append(pwm,files.values[:,2])\n",
    "    angle = np.append(angle,np.zeros(100))\n",
    "    angle = np.append(angle,files.values[:,3])\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "window = 100\n",
    "\n",
    "#For each element of training set, we have we have \"window\" previous training set elements \n",
    "for i in range(window,pwm.shape[0]):\n",
    "    X_train.append(pwm[i-window:i])\n",
    "    Y_train.append(angle[i])\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "# Separate the values in train, validation and test data/label\n",
    "train_data, val_data, test_data = [],[],[]\n",
    "train_label, val_label, test_label = [],[],[]\n",
    "\n",
    "train_lenght = int(len(X_train)*3/5)\n",
    "val_lenght = int(len(X_train)*4/5)\n",
    "\n",
    "# Use 3/5 of the total data set for training \n",
    "# and 1/5 for validation and testing.\n",
    "\n",
    "for i in X_train[:train_lenght]:\n",
    "    train_data.append(i)\n",
    "    train_label.append(i)\n",
    "\n",
    "for i in X_train[train_lenght:val_lenght]:\n",
    "    val_data.append(i)\n",
    "    val_label.append(i)\n",
    "    \n",
    "for i in X_train[val_lenght:]:\n",
    "    test_data.append(i)\n",
    "    test_label.append(i)\n",
    "\n",
    "train_data, val_data, test_data = np.array(train_data), np.array(val_data), np.array(test_data)\n",
    "train_label, val_label, test_label = np.array(train_label), np.array(val_label), np.array(test_label)\n",
    "\n",
    "print('El total de datos de entrenamiento es: ', len(train_data), flush=True)\n",
    "print('El total de datos de validación es: ', len(val_data), flush=True)\n",
    "print('El total de datos de prueba es: ', len(test_data), flush=True)\n",
    "\n",
    "# Reshape the arrays (n,window,1). Where n is the total amount of data in the array\n",
    "train_data = np.reshape(train_data,(train_data.shape[0],train_data.shape[1],1))\n",
    "val_data = np.reshape(val_data,(val_data.shape[0],val_data.shape[1],1))\n",
    "test_data = np.reshape(test_data,(test_data.shape[0],test_data.shape[1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63423229",
   "metadata": {},
   "source": [
    "******************* Neural Network *******************\n",
    "Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ad37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(64, input_shape=(X_train.shape[1],1),return_sequences=True))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(GRU(64, input_shape=(X_train.shape[1],1),return_sequences=True))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(TimeDistributed(Dense(1))) # There is no difference between this and model.add(Dense(1))...\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mse','acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59b4162",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091be107",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_label,\n",
    "                    epochs=500, batch_size=8,\n",
    "                    validation_data = (val_data,val_label),\n",
    "                    verbose=2)\n",
    "# Prediction\n",
    "testPredict = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d7a11",
   "metadata": {},
   "source": [
    "Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b35b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(testX,output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'GRU_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151d16e",
   "metadata": {},
   "source": [
    "Plot the predicted and \"true\" output and plot training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_plot=range(1,time+2)\n",
    "plt.figure()\n",
    "plt.plot(time_plot,testPredict[0,:,0], label='Real output')\n",
    "plt.plot(time_plot,output_test[0,:],'r', label='Predicted output')\n",
    "plt.xlabel('Discrete time steps')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(1,len(loss)+1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss,'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss,'r', label='Validation loss')\n",
    "plt.title('Training and validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
