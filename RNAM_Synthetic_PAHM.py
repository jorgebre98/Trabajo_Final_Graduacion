# ************************************************************************************************************************* #
#                                           Copyright (C) 2022 Jorge Brenes Alfaro.
#                                           EL5617 Trabajo Final de Graduación.
#                                           Escuela de Ingeniería Electrónica.
#                                           Tecnológico de Costa Rica.
# *************************************************************************************************************************** #
#
#   This file contains the mimetic neural network (MNN) that uses the empirical mathematical
#   model of the PAHM.
#
#   The data used for training, validation and testing are randomly generated, while their
#   labels (angles) are generated by mathematical model. The angle values are normalized
#   between -1 and 1 for better network performance, the data are prepared in the form of
#   tensor, necessary for the GRU layer. Finally, the network model is developed, training,
#   prediction and evaluation of the model are performed.

# Libraries to process data
import numpy as np
import matplotlib.pyplot as plt
from numpy.linalg import inv

#Libraries to create de MNN
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.layers import Dense, GRU, TimeDistributed, Dropout
from Synthetic_PAHM import dynamic_model

import wandb
from wandb.keras import WandbCallback

#   The parameters are archived in Weights and Biases (W&B), as well as the results of the
#   execution for further evaluation.
wandb.login()

wandb.init(project="Synthetic PAHM", 
           entity="mimetic-rna", 
           name='Synthetic PAHM2',
           resume='Allow',
           notes='Se entrena la sintética con 64 neuronas y batch de 32',
           id='Synthetic PAHM2')
wandb.config = {
    "epochs": 3500,
    "batch_size": 32,
    "units": 64,
    "learning_rate":0.001,
    "Dropout": 0.35
}

#   This function is used to normalized values.
def normalizer(angle, action):
    min_val, max_val = -120, 120
    if action == 'norm':
        return (angle - min_val)/(max_val - min_val)
    else:
        return angle*(max_val - min_val)+min_val

#   This function plots training loss vs validation loss. 
def plot_loss (history):
    plt.figure(figsize = (10, 6))
    plt.plot(history.history['loss'], label='Train loss')
    plt.plot(history.history['val_loss'], label='Validation loss')
    plt.title('Model Train vs Validation Loss for GRU network')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(loc='upper right')
    plt.savefig('loss_Synth_norm2.png')

#   This function plots the actual output vs the output predicted by the model. 
def plot_future(prediction, y_test):
    plt.figure(figsize=(10, 6))
    range_future = np.arange(prediction.shape[1])
    plt.plot(range_future, y_test[0,:], label='Test data', color = [1, 0.502, 0])
    plt.plot(range_future, prediction[0,:], label='Prediction',  color = [0.0502, 0.706, 0.949])
    plt.title('Predicted and true output')
    plt.xlabel('Tiempo (ms)')
    plt.ylabel('Ángulo (°)')
    plt.legend(loc='lower right')
    plt.savefig('Predict_Synth_norm2.png')

#   This function calculates performance metrics for regression problems.    
def evaluate_prediction(predictions, actual):
    errors = predictions - actual
    mse = np.square(errors).mean()
    rmse = np.sqrt(mse)
    mae = np.abs(errors).mean()
    print('GRU:')
    print('Mean Absolute Error: {:.4f}%'.format(mae))
    print('Mean Square Error: {:.4f}%.'.format(mse))
    print('Root Mean Square Error: {:.4f}%'.format(rmse))

#   Definition of the model in continuous time.
A = np.matrix([[0, 1],[-17.97, -0.3801]])
B = np.matrix([[0],[2965]])
C = np.matrix([[1, 0]])

# Number of time samples
time = 120000
sampling = 0.02

print('******************* Creating the Dataset *******************', flush=True)
# ***************** Create the training data *****************
input_seq_train = np.random.rand(time,1)*0.25 # Input sequence for the simulation
x0_train = np.random.rand(2,1)*0.25 # Initial state for simulation

state, train_label = dynamic_model(A, B, C,
                                    x0_train, input_seq_train, 
                                    time ,sampling) # Simulate the dynamics 

train_label = np.reshape(train_label.T, (1, train_label.T.shape[0], 1)) # Label train data

input_seq_train = np.reshape(input_seq_train,(input_seq_train.shape[0], 1))
tmp_train = np.concatenate((input_seq_train, np.zeros(shape = (input_seq_train.shape[0], 1))), axis=1)
tmp_train = np.concatenate((x0_train.T,tmp_train), axis = 0)
train_data = np.reshape(tmp_train, (1, tmp_train.shape[0], tmp_train.shape[1])) # Train Data

# ***************** Create the validation data *****************
input_seq_val = np.random.rand(time,1)*0.25 # New input sequence
x0_val=np.random.rand(2,1)*0.25

state_val, val_label = dynamic_model(A, B, C,
                                     x0_val, input_seq_val, 
                                     time ,sampling) 

val_label = np.reshape(val_label.T,(1,val_label.T.shape[0],1)) # Label validation data

input_seq_val = np.reshape(input_seq_val,(input_seq_val.shape[0],1))
tmp_val = np.concatenate((input_seq_val, np.zeros(shape = (input_seq_val.shape[0],1))), axis=1)
tmp_val = np.concatenate((x0_val.T,tmp_val), axis = 0)
val_data = np.reshape(tmp_val, (1,tmp_val.shape[0],tmp_val.shape[1])) # Validation Data

#   ***************** Create the test data *****************
time = 200
input_seq_test = np.random.rand(time,1)*0.25
x0_test = np.random.rand(2,1)*0.25

state_test, test_label = dynamic_model(A, B , C,
                                      x0_test, input_seq_test, 
                                      time ,sampling)

test_label = np.reshape(test_label.T,(1,test_label.T.shape[0],1)) # Label test data

input_seq_test = np.reshape(input_seq_test,(input_seq_test.shape[0],1))
tmp_test = np.concatenate((input_seq_test, np.zeros(shape=(input_seq_test.shape[0],1))), axis=1)
tmp_test = np.concatenate((x0_test.T,tmp_test), axis=0)
test_data = np.reshape(tmp_test, (1,tmp_test.shape[0],tmp_test.shape[1])) # Test data

train_label = normalizer(train_label, 'norm')
val_label = normalizer(val_label, 'norm')
test_label = normalizer(test_label, 'norm')

print('Total train data is: ', train_data.shape[0], flush=True)
print('Total validation data is: ', val_data.shape[0], flush=True)
print('Total testing data is:: ', test_data.shape[0], flush=True)
print('\nTrain data shape is: ', train_data.shape, flush=True)
print('Validation data shape is: ', val_data.shape, flush=True)
print('Testing data shape is:: ', test_data.shape, flush=True)
print('Train label shape is: ', train_label.shape, flush=True)
print('Validation label shape is: ', val_label.shape, flush=True)
print('Testing label shape is:: ', test_label.shape, flush=True)

#   ***************** Neuronal Network *****************
#   Model Creation
model = Sequential()
model.add(GRU(units=wandb.config['units'], input_shape=(None,train_data.shape[2]),return_sequences=True))
#model.add(Dropout(wandb.config['Dropout']))

#   Hidden Layer
#model.add(GRU(units=wandb.config['units']))
#model.add(Dropout(wandb.config['Dropout']))
model.add(Dense(1))

#   Compile model
model.compile(optimizer = RMSprop(learning_rate = wandb.config['learning_rate']), 
              loss = 'mean_absolute_error', metrics = ['mae'])
model.summary()

#   Train model
history = model.fit(train_data, train_label ,
                    epochs = wandb.config['epochs'], batch_size = wandb.config['batch_size'], 
                    validation_data = (val_data, val_label),
                    verbose = 1, callbacks=[WandbCallback(save_model=False)])
model.save('Synthetic_PAHM_norm.h5')

#   Model Prediction
testPredict = model.predict(test_data)

#   Desnormalizing Values
testPredict = normalizer(testPredict, '')
test_label = normalizer(test_label, '')

#   Model Evaluate
plot_loss(history)
plot_future(testPredict, test_label)
evaluate_prediction(testPredict, test_label)
